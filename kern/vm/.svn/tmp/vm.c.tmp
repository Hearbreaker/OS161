#include <types.h>
#include <kern/errno.h>
#include <lib.h>
#include <thread.h>
#include <curthread.h>
#include <addrspace.h>
#include <vm.h>
#include <machine/spl.h>
#include <machine/tlb.h>
#include <uio.h>
#include <vnode.h>

//??????????????QUESTIONS
//heapend and heapstart value

/*
 * Dumb MIPS-only "VM system" that is intended to only be just barely
 * enough to struggle off the ground. You should replace all of this
 * code while doing the VM assignment. In fact, starting in that
 * assignment, this file is not included in your kernel!
 */

/* under dumbvm, always have 48k of user stack */
#define DUMBVM_STACKPAGES    12

//This is only a global struct to for PM available
static struct {
    u_int32_t  numofpage;
    u_int32_t  FirstPageAdd;
    u_int32_t  numofpageleft;
    struct page* firstPage;
}coremap;


void
vm_bootstrap(void)
{
  //************coremap start
  u_int32_t firstadd , lastadd;
  u_int32_t firstpage, lastpage , totalpage , mapsize, mappage;
  //To get the physical memory size from low to high
  ram_getsize(&firstadd, &lastadd);

  firstpage=(firstadd)&0xfffff000; //mask out addr
  lastpage=(lastadd)&0xfffff000;
  //total number of pages available in the RAM
  totalpage = (lastpage >> 12)-(firstpage >> 12); 

  //since each coremap entry may map to one or more pages, so the 
  //worst case is one to one relation

  //used RAM for the coremap
  //mappage is number of pages that coremap used in the RAM
  mapsize=totalpage*sizeof(struct page); 
  mappage=mapsize/4096+1; 

  //will be upated
  //firstadd is the first PM addr
  coremap.FirstPageAdd = firstpage;
  coremap.numofpage = totalpage;

  //storing the first coremap entry vaddr
  coremap.firstPage = (struct page*)PADDR_TO_KVADDR(firstadd);

  //clear the content on coremap addr
  //bzero(coremap.firstPage, coremap.numofpage * sizeof(struct page)); 
  
  //set up the starting point of available phyical address
  coremap.FirstPageAdd=coremap.FirstPageAdd+(mappage<<12); 
  
  //updating number of page available in physical memory
  coremap.numofpage=coremap.numofpage-mappage;
  
  coremap.numofpageleft = coremap.numofpage;
  ///coremap first page addr, mappage may not  needed
  //coremap.firstPage = coremap.firstPage;

  /* Do nothing. */
}

//static
paddr_t getppages(unsigned long npages)
{
	/////////////////////////////A2
	// int spl;
	// paddr_t addr;

	// spl = splhigh();

	// addr = ram_stealmem(npages);
	
	// splx(spl);
	// return addr;

	///////////////////////////A3
	int spl;
	paddr_t addr;

	spl = splhigh();
	//first enter to find a physical page
	if(coremap.firstPage == NULL){
		addr = ram_stealmem(npages);
		splx(spl);
		return addr;
	}
	else{
		//the starting vaddr of coremap and increment by coremap entry
		struct page* start=coremap.firstPage;
		//This is the starting 
		struct page* freepagestart;


		int count=0;
		//check the free phyiscal page
		while (start<(coremap.firstPage + coremap.numofpage))
		{
			while(start<(coremap.firstPage + coremap.numofpage) && !(start->allocated==0))
			{
				//increment by "1" coremap entry and check allocated or not
				start++;   //skip all allocated pages
				
			}
			freepagestart=start; //Store the starting point of the free page
			//keep requiring the consecutive pages
			while (start<(coremap.firstPage + coremap.numofpage) && (start->allocated==0)) 
			{
				count++;
				start++;
			}
			//find enough memory and initialize the coremap entries
			if (count>=npages)
			{	
				//find the physical memory addr(base + # of pages)
				//1 coremap maps 1 physical memory
				addr=(paddr_t)(coremap.FirstPageAdd + ((freepagestart) - coremap.firstPage) * 4096);
				//how many page used consecutively from the starting point
				freepagestart->consec = npages;
				
				//initailize the n coremap entries
				for(count=0; count<npages; count++)
				{
    				freepagestart[count].allocated = 1;
    				freepagestart[count].as = NULL;
					freepagestart[count].isKernelPage = 1;
				}
				//updating # of pages left in the PM	
				coremap.numofpageleft =coremap.numofpageleft-npages;
				//finish and return physical addr
				splx(spl);
				return addr;
				} 
		}

	}
	splx(spl);
	return 0;
}

/* Allocate/free some kernel-space virtual pages */
vaddr_t 
alloc_kpages(int npages)
{
	paddr_t pa;
	pa = getppages(npages);
	if (pa==0) {
		return 0;
	}
	return PADDR_TO_KVADDR(pa);
}

void 
free_kpages(vaddr_t addr)
{
	/* nothing */


	//KVADDR_TO_PADDR(addr);
	////////////////////////////////A3
	int spl;
	spl = splhigh();

	//convert into the paddr
	paddr_t paddr;
	paddr=addr-MIPS_KSEG0;	
  	assert(paddr == (paddr&0xfffff000));
	
	//get starting coremap entry (base + # of physical page blocks)
	struct page* currentpage;
	currentpage =(coremap.firstPage + (((u_int32_t)(paddr) - coremap.FirstPageAdd) >> 12));	
	int i = 0; 
	int npage;

  	assert(currentpage != NULL);

  	assert(currentpage +  currentpage->consec <= coremap.firstPage + coremap.numofpage);
	
	//free the consecutive coremap entries space starting from current page 
    for (i = 0; i < currentpage->consec; i++) 
	{
    	assert(currentpage != NULL);
    	assert(currentpage[i].isKernelPage == 1);
    	assert(currentpage[i].allocated == 1);
		currentpage[i].allocated=0; 
		currentpage[i].isKernelPage =0; 
		//kprintf("i freed %d pages\n",i+1);
	}	
	//updating page left in physical memory
	coremap.numofpageleft=coremap.numofpageleft+currentpage->consec;
 	currentpage->consec = 0;
	splx(spl);
	return;
}

int getOnePhysicalFrame (struct page** freePageFrame) {
  assert(*freePageFrame == NULL);
  assert(curspl > 0);
  struct page* pageEntry = coremap.firstPage;
  while (pageEntry < (coremap.firstPage + coremap.numofpage)) {
    if ((pageEntry < (coremap.firstPage + coremap.numofpage)) && pageEntry->allocated == 0) {
      pageEntry->allocated = 1;
      *freePageFrame = pageEntry;
      return 0;
    }
    pageEntry++;
  }
  return -1;
}

int addMapping (struct addrspace* as, vaddr_t faultaddress, struct page** freePageFrame) {
  assert((*freePageFrame)->allocated == 1);
  assert((faultaddress & PAGE_FRAME) == faultaddress);
  assert(as != NULL);

  u_int32_t lvlOnePageTableIndex = (unsigned)faultaddress>>22;
  u_int32_t lvlTwoPageTableIndex = ((unsigned)faultaddress>>12)&0x3ff;
  struct lvonepagetable* lvlOnePageTable = &(as->pageTable);
  struct lvtwopagetable* lvlTwoPageTable = lvlOnePageTable->lvtwo[lvlOnePageTableIndex];

  //allocate one master page linked to one secondary page table with 1024 entries
  //no master page table to allocate, then create a new one
  if (lvlTwoPageTable == NULL) {
    lvlTwoPageTable = kmalloc(sizeof(struct lvtwopagetable));
    if (lvlTwoPageTable == NULL) return ENOMEM;
    bzero(lvlTwoPageTable, sizeof(struct lvtwopagetable));
    struct pagetableentry* pte = lvlTwoPageTable->entry;
    int i;
    //all entry are invaild with no pages attached
    for (i = 0; i < 1024; i++) {
      pte[i].valid = 0;
      pte[i].pagePtr = NULL;
    }
    //link master table to as
    lvlOnePageTable->lvtwo[lvlOnePageTableIndex] = lvlTwoPageTable;
  }
  //linked a page entry to master pagetable
  struct pagetableentry* pte = &(lvlTwoPageTable->entry[lvlTwoPageTableIndex]);
  /* Not sure if these signals are correct */
  /* Everything is readable and everything is writable */
  pte->dirty = 1;
  pte->writable = 1;
  //used page and set vaild bit to '1'
  pte->valid = 1;
  //add the physical addr
  pte->pageframeadd = coremap.FirstPageAdd + (*freePageFrame - coremap.firstPage)*PAGE_SIZE;
  assert(pte->pageframeadd == (pte->pageframeadd)&(0x3ff));
  //kprintf("pte->pageframeadd: %x\n",pte->pageframeadd);
  //linked a coremap entry to page entry
  pte->pagePtr = *freePageFrame; // Perhaps redundant pointer
  assert((*freePageFrame)->allocated == 1); // Extra assertions to make sure
  (*freePageFrame)->isKernelPage = 0;
  (*freePageFrame)->as = as; // Perhaps another redundant pointer
  (*freePageFrame)->pte = pte;
  (*freePageFrame)->va = faultaddress; // This is stored unshifted
  (*freePageFrame)->consec = 1;
  return 0;
}

//find a page in stack and set up a mapping to page in PM
int increaseStack (struct addrspace* as, vaddr_t faultaddress, paddr_t* _paddr) {
  assert((faultaddress & PAGE_FRAME) == faultaddress);
  assert(as != NULL);

  struct page* freePageFrame = NULL;
  int paddr;
  //get one physical page
  if (getOnePhysicalFrame(&freePageFrame)) {
    assert(freePageFrame == NULL);
    return -1; // fail
  }
  assert(freePageFrame != NULL);
  //map one VA to a coremap entry in PM
  if (addMapping(as, faultaddress, &freePageFrame)) {
    return -1; // fail
  }

  //find a physical addr in PM
  paddr = coremap.FirstPageAdd + (freePageFrame - coremap.firstPage)*4096;
  bzero((void*)PADDR_TO_KVADDR(paddr), PAGE_SIZE);
  //Decrease the stackpointer
  as->stackEnd = as->stackEnd - PAGE_SIZE;
  //return paddr
  *_paddr = paddr;
  return 0;
}


int increaseHeap (struct addrspace* as, vaddr_t faultaddress, paddr_t* _paddr) {
  assert((faultaddress & PAGE_FRAME) == faultaddress);
  assert(as != NULL);

  struct page* freePageFrame = NULL;
  int paddr;
  if (getOnePhysicalFrame(&freePageFrame)) {
    assert(freePageFrame == NULL);
    return -1; // fail
  }
  if (addMapping(as, faultaddress, &freePageFrame)) return -1; // fail

  paddr = coremap.FirstPageAdd + (freePageFrame - coremap.firstPage)*4096;
  bzero((void*)PADDR_TO_KVADDR(paddr), PAGE_SIZE);
  //as->heapend += PAGE_SIZE;
  *_paddr = paddr;
  return 0;
}


int onStack (struct addrspace* as, vaddr_t faultaddress) {
  // kprintf("This is userstack: %x\n",USERSTACK);
  // kprintf("This is stackEnd: %x\n",as->stackEnd);
  if ((USERSTACK - as->stackEnd) >= /*5001216*/0x500000) {
    return 1; // Here, we are limiting the maximum stack size to be 5 megabytes
  }
  if (faultaddress <= USERSTACK && faultaddress >= as->stackEnd) return 1;
  //this is bigger range then PA can fill into
  if (faultaddress <= USERSTACK && faultaddress >= (as->stackEnd - PAGE_SIZE)) return 1;
  return 0;
}

int onHeap (struct addrspace* as, vaddr_t faultaddress) {
  if (faultaddress >= as->heapstart && faultaddress <= as->heapend) return 1;
  return 0;
}

//after this function, it only maps VA to PA and do not find a free PM for VA
//return -1 1, master table does not found 2, level two invaild
int findFrame (struct addrspace* as, vaddr_t faultaddress, paddr_t* _paddr) {
  // assert((faultaddress & PAGE_FRAME) == faultaddress);
  // assert(as != NULL);

  //This is the first PageTable index(first 10 bits of 32 bits)
  u_int32_t lvlOnePageTableIndex = (unsigned)faultaddress>>22;

  //This is second 10 bits of 32 bit VA (for secondary pagetable)
  u_int32_t lvlTwoPageTableIndex = ((unsigned)faultaddress>>12)&0x3ff;
  struct lvonepagetable* lvlOnePageTable = &(as->pageTable);
  //lvtwo is an array of first level page table entries
  //lvtwopagetable is a secondary page table pointer and point to 1024 secondary page table entrys
  struct lvtwopagetable* lvlTwoPageTable = lvlOnePageTable->lvtwo[lvlOnePageTableIndex];

  if (lvlTwoPageTable == NULL) return -1;
  //The level two page table pointing to one page entry
  //pte storing one physical page
  struct pagetableentry* pte = &(lvlTwoPageTable->entry[lvlTwoPageTableIndex]);
  if (pte->valid == 0) return -1;
  //kprintf("This is the faultaddress: %x\n",faultaddress);
  //kprintf("This is the matching address: %x\n", pte->pagePtr->va);
  assert(pte->pageframeadd == (pte->pageframeadd)&(0x3ff));
  *_paddr = pte->pageframeadd;
  //kprintf("FOUND\n");
  return 0;
}

int load (struct addrspace* as, vaddr_t faultaddress, u_int32_t base, paddr_t paddr, u_int32_t vaddr, off_t offset, int memSize, int fileSize) {
  assert((faultaddress&PAGE_FRAME) == faultaddress);
  //kprintf("This is the faultaddress: %x\n",faultaddress);
  struct uio u;
  int result;
  size_t fillamt;
  struct vnode* v = as->v;


  if (fileSize > memSize) {
		kprintf("ELF: warning: segment filesize > segment memsize\n");
		//fileSize = memSize;
  }

  int pageOffset = (faultaddress - base)/PAGE_SIZE;
  assert(pageOffset >= 0);
  int check = fileSize - (pageOffset + 1)*PAGE_SIZE;
  int residue;
  if (check >= 0) residue = PAGE_SIZE;
  //may be useless
  else if (check < -PAGE_SIZE) residue = 0;
  else if (check < 0) residue = fileSize - pageOffset*PAGE_SIZE;
  else panic("Shouldn't be here\n");

	u.uio_iovec.iov_kbase = PADDR_TO_KVADDR(paddr);
	//passing data size is one page at one time 
	u.uio_iovec.iov_len = PAGE_SIZE; // PAGE_SIZE
	//actual reading data size
	u.uio_resid = residue;
	u.uio_offset = pageOffset*PAGE_SIZE+offset;
	u.uio_segflg = UIO_SYSSPACE;
	u.uio_rw = UIO_READ;
	u.uio_space = NULL;

	result = VOP_READ(v, &u);
	if (result) {
		return result;
	}

	if (u.uio_resid != 0) {
		/* short read; problem with executable? */
		kprintf("ELF: short read on segment - file truncated?\n");
		return ENOEXEC;
	}

#if 0
  memSize = (pageOffset + 1)*PAGE_SIZE;
	fillamt = memSize - fileSize;
	if (fillamt > 0) {
		DEBUG(DB_EXEC, "ELF: Zero-filling %lu more bytes\n", 
		      (unsigned long) fillamt);
		u.uio_resid += fillamt;
		result = uiomovezeros(fillamt, &u);
	}
#endif
	
	return result;

}



int loadPage (struct addrspace* as, vaddr_t faultaddress, paddr_t* _paddr, int region) {
  if (region == 1) assert((faultaddress >= as->as_vbase1) && (faultaddress < as->as_vbase1 + as->as_npages1 * PAGE_SIZE));
  if (region == 2) assert((faultaddress >= as->as_vbase2) && (faultaddress < as->as_vbase2 + as->as_npages2 * PAGE_SIZE));
  assert((faultaddress & PAGE_FRAME) == faultaddress);
  assert(as != NULL);
  assert(as->v != NULL);

  struct page* freePageFrame = NULL;
  int paddr;
  if (getOnePhysicalFrame(&freePageFrame)) {
    assert(freePageFrame == NULL);
    //kprintf("failed to getOnePhysicalFrame\n");
    return -1; // fail
  }
  assert(freePageFrame != NULL);
  if (addMapping(as, faultaddress, &freePageFrame)) {
    //kprintf("failed to add mapping\n");
    return -1; // fail
  }
  assert(freePageFrame != NULL);
  assert(freePageFrame->va == faultaddress);
  paddr = coremap.FirstPageAdd + (freePageFrame - coremap.firstPage)*4096;
  bzero((void*)PADDR_TO_KVADDR(paddr), PAGE_SIZE);

  /* start */
  u_int32_t p_vaddr1 = as->progheader[0].p_vaddr;
  u_int32_t p_vaddr2 = as->progheader[1].p_vaddr;
  //struct vnode* v = as->v;
  size_t offset1 = as->offset1;
  size_t offset2 = as->offset2;
  int memSize1 = as->memSize1;
  int memSize2 = as->memSize2;
  int fileSize1 = as->fileSize1;
  int fileSize2 = as->fileSize2;
  int base1 = as->as_vbase1;
  int base2 = as->as_vbase2;

  int result = -1;
  if (faultaddress >= p_vaddr1 && faultaddress < p_vaddr1 + memSize1) {
    //panic("out1\n");
    assert(region == 1);
    result = load(as, faultaddress, base1, paddr, p_vaddr1, offset1, memSize1, fileSize1);
  } else if (faultaddress >= p_vaddr2 && faultaddress < p_vaddr2 + memSize2) {
    //panic("out2\n");
    assert(region == 2);
    result = load(as, faultaddress, base2, paddr, p_vaddr2, offset2, memSize2, fileSize2);
  } else {
    panic("I shouldn't be here.");
  }
  if (result) return result;
  /* start --END-- */

  *_paddr = paddr;
  return 0;
}





int
vm_fault(int faulttype, vaddr_t faultaddress)
{	

	vaddr_t vbase1, vtop1, vbase2, vtop2, stackbase, stacktop;
	paddr_t paddr;
	int i;
	u_int32_t ehi, elo;
	struct addrspace *as;
	int spl;

	spl = splhigh();

	faultaddress &= PAGE_FRAME;

	DEBUG(DB_VM, "dumbvm: fault: 0x%x\n", faultaddress);

	switch (faulttype) {
	    case VM_FAULT_READONLY:
		/* We always create pages read-write, so we can't get this */
		panic("dumbvm: got VM_FAULT_READONLY\n");
	    case VM_FAULT_READ:
	    case VM_FAULT_WRITE:
		break;
	    default:
		splx(spl);
		return EINVAL;
	}

	as = curthread->t_vmspace;
	if (as == NULL) {
		/*
		 * No address space set up. This is probably a kernel
		 * fault early in boot. Return EFAULT so as to panic
		 * instead of getting into an infinite faulting loop.
		 */
		return EFAULT;
	}


  // vbase1 = as->as_vbase1;
  // vtop1 = vbase1 + as->as_npages1 * PAGE_SIZE;
  // vbase2 = as->as_vbase2;
  // vtop2 = vbase2 + as->as_npages2 * PAGE_SIZE;
  // stackbase = USERSTACK - DUMBVM_STACKPAGES * PAGE_SIZE;
  // stacktop = USERSTACK;
 


	///////////////////////////A3
	//if we can find a translation, it will directly return a paddr, ret 0
	//other to will set a mapping and a free page, ret 0
	//if fails, ret -1
	int result = -1;
  	result = findFrame(as, faultaddress, &paddr);
  	if (result) {
	    if (onStack(as, faultaddress)) {
	     // kprintf("updating stack\n");
	      result = increaseStack(as, faultaddress, &paddr);
	    } 
      else if (onHeap(as, faultaddress)) {
	      kprintf("updating heap\n");
	      result = increaseHeap(as, faultaddress, &paddr);
	    }
      else {
	      // kprintf("updating data and text\n");
<<<<<<< .mine
	      if ((faultaddress >= as->as_vbase1) && (faultaddress < (as->as_vbase1 + as->as_npages1 * PAGE_SIZE))) {
	        result = loadPage(as, faultaddress, &paddr, 1);
	      } else if ((faultaddress >= as->as_vbase2) && (faultaddress < (as->as_vbase2 + as->as_npages2 * PAGE_SIZE))) {
	        result = loadPage(as, faultaddress, &paddr, 2);
	      } 
	    	//else {
	#if 0
	        kprintf("This is as->as_vbase1: %x\n",as->as_vbase1);
	        kprintf("This is as->as_vbase2: %x\n",as->as_vbase2);
	        kprintf("This is the fault address: %x\n",faultaddress);
	#endif
	      	}
=======
	      // if ((faultaddress >= as->as_vbase1) && (faultaddress < (as->as_vbase1 + as->as_npages1 * PAGE_SIZE))) {
	      //   result = loadPage(as, faultaddress, &paddr, 1);
	      // } else if ((faultaddress >= as->as_vbase2) && (faultaddress < (as->as_vbase2 + as->as_npages2 * PAGE_SIZE))) {
	      //   result = loadPage(as, faultaddress, &paddr, 2);
	      // } 

 //        else {
	// #if 0
	//         kprintf("This is as->as_vbase1: %x\n",as->as_vbase1);
	//         kprintf("This is as->as_vbase2: %x\n",as->as_vbase2);
	//         kprintf("This is the fault address: %x\n",faultaddress);
	// #endif
	//       	}
      }
>>>>>>> .r29
    }



  if (result) {
#if 1
    kprintf("This is userstack: %x\n",USERSTACK);
    kprintf("This is stackEnd: %x\n",as->stackEnd);
    kprintf("This is stackEnd- page size: %x\n",as->stackEnd - PAGE_SIZE);
    kprintf("THIS IS FAULINT LIKE CRAXY \n");
    kprintf("This is USERSTACK - as->heapstart: %x\n",USERSTACK-as->heapstart);
    kprintf("This is USERSTACK - as->stackEnd: %x\n",USERSTACK - as->stackEnd);
    kprintf("This is as->heapend - as->heapstart: %x\n",as->heapend-as->heapstart);
    kprintf("This is as->as_vbase1: %x\n",as->as_vbase1);
    kprintf("This is as->as_vbase2: %x\n",as->as_vbase2);
    kprintf("This is the fault address: %x\n",faultaddress);
    kprintf("This is as->as_vbase1 + as->as_npages1 * PAGE_SIZE: %x\n",as->as_vbase1 + as->as_npages1 * PAGE_SIZE);
    kprintf("This is as->as_vbase2 + as->as_npages2 * PAGE_SIZE: %x\n",as->as_vbase2 + as->as_npages2 * PAGE_SIZE);
    kprintf("DONE\n");
#endif
    splx(spl);
    return EFAULT;
  }
	/* Assert that the address space has been set up properly. */
	// assert(as->as_vbase1 != 0);
	// assert(as->as_pbase1 != 0);
	// assert(as->as_npages1 != 0);
	// assert(as->as_vbase2 != 0);
	// assert(as->as_pbase2 != 0);
	// assert(as->as_npages2 != 0);
	// assert(as->as_stackpbase != 0);
	// assert((as->as_vbase1 & PAGE_FRAME) == as->as_vbase1);
	// assert((as->as_pbase1 & PAGE_FRAME) == as->as_pbase1);
	// assert((as->as_vbase2 & PAGE_FRAME) == as->as_vbase2);
	// assert((as->as_pbase2 & PAGE_FRAME) == as->as_pbase2);
	// assert((as->as_stackpbase & PAGE_FRAME) == as->as_stackpbase);

	// vbase1 = as->as_vbase1;
	// vtop1 = vbase1 + as->as_npages1 * PAGE_SIZE;
	// vbase2 = as->as_vbase2;
	// vtop2 = vbase2 + as->as_npages2 * PAGE_SIZE;
	// //stackbase = USERSTACK - DUMBVM_STACKPAGES * PAGE_SIZE;
	// //stacktop = USERSTACK;

	// if (faultaddress >= vbase1 && faultaddress < vtop1) {
	// 	//paddr = (faultaddress - vbase1) + as->as_pbase1;
	// }
	// else if (faultaddress >= vbase2 && faultaddress < vtop2) {
	// 	//paddr = (faultaddress - vbase2) + as->as_pbase2;
	// }
	// else if (faultaddress >= stackbase && faultaddress < stacktop) {
	// 	//paddr = (faultaddress - stackbase) + as->as_stackpbase;
	// }
	// else {
	// 	splx(spl);
	// 	return EFAULT;
	// }

	/* make sure it's page-aligned */
	assert((paddr & PAGE_FRAME)==paddr);

	for (i=0; i<NUM_TLB; i++) {
		TLB_Read(&ehi, &elo, i);
		if (elo & TLBLO_VALID) {
			continue;
		}
		ehi = faultaddress;
		elo = paddr | TLBLO_DIRTY | TLBLO_VALID;
		DEBUG(DB_VM, "dumbvm: 0x%x -> 0x%x\n", faultaddress, paddr);
		TLB_Write(ehi, elo, i);
		splx(spl);

		return 0;
	}

	kprintf("dumbvm: Ran out of TLB entries - cannot handle page fault\n");
	splx(spl);
	return EFAULT;
}

void freePage (struct page* pagePtr) {
  assert(pagePtr != NULL);
  assert(pagePtr->consec == 1);
  assert(pagePtr->isKernelPage == 0);
  assert(pagePtr->as != NULL);
  assert(pagePtr->pte != NULL);
  pagePtr->as = NULL;
  pagePtr->pte = NULL;
  pagePtr->va = 0;
  pagePtr->allocated = 0;
  pagePtr->consec = 0;
  pagePtr->isKernelPage = 0;
}
